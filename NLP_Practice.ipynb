{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b96da812",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kawba\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import nltk.corpus\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6aa4797",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6980c2fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'love', 'Naheem', '.', 'He', 'is', 'my', 'favourite', 'human']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#string\n",
    "raw_text = 'I love Naheem. He is my favourite human'\n",
    "\n",
    "#tokenize\n",
    "text_tokens = word_tokenize(raw_text)\n",
    "text_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d421fafb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, 10)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#type of tokens\n",
    "type(text_tokens), len(text_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04978aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#frequency of tokens\n",
    "\n",
    "from nltk.probability import FreqDist\n",
    "freq_dist = FreqDist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1aae98fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({' ': 8, 'e': 3, 'i': 3, 'm': 3, 'a': 3, 'o': 2, 'v': 2, 't': 2, 'h': 2, 's': 2, ...})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in raw_text:\n",
    "    freq_dist[i] = freq_dist[i]+1\n",
    "freq_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01b81308",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(' ', 8),\n",
       " ('e', 3),\n",
       " ('i', 3),\n",
       " ('m', 3),\n",
       " ('a', 3),\n",
       " ('o', 2),\n",
       " ('v', 2),\n",
       " ('t', 2),\n",
       " ('h', 2),\n",
       " ('s', 2)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#most common tokens\n",
    "\n",
    "top_ten = freq_dist.most_common(10)\n",
    "top_ten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e3644a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bigrams, trigram, ngrams\n",
    "\n",
    "new_text = \"I'm distracted from this movie. Yet, i need to get work done. I wonder why one cannot give\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "145062d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " \"'m\",\n",
       " 'distracted',\n",
       " 'from',\n",
       " 'this',\n",
       " 'movie',\n",
       " '.',\n",
       " 'Yet',\n",
       " ',',\n",
       " 'i',\n",
       " 'need',\n",
       " 'to',\n",
       " 'get',\n",
       " 'work',\n",
       " 'done',\n",
       " '.',\n",
       " 'I',\n",
       " 'wonder',\n",
       " 'why',\n",
       " 'one',\n",
       " 'can',\n",
       " 'not',\n",
       " 'give']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_text_tokens = word_tokenize(new_text)\n",
    "new_text_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "95c95a92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', \"'m\"),\n",
       " (\"'m\", 'distracted'),\n",
       " ('distracted', 'from'),\n",
       " ('from', 'this'),\n",
       " ('this', 'movie'),\n",
       " ('movie', '.'),\n",
       " ('.', 'Yet'),\n",
       " ('Yet', ','),\n",
       " (',', 'i'),\n",
       " ('i', 'need'),\n",
       " ('need', 'to'),\n",
       " ('to', 'get'),\n",
       " ('get', 'work'),\n",
       " ('work', 'done'),\n",
       " ('done', '.'),\n",
       " ('.', 'I'),\n",
       " ('I', 'wonder'),\n",
       " ('wonder', 'why'),\n",
       " ('why', 'one'),\n",
       " ('one', 'can'),\n",
       " ('can', 'not'),\n",
       " ('not', 'give')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(nltk.bigrams(new_text_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "94e67d98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', \"'m\", 'distracted'),\n",
       " (\"'m\", 'distracted', 'from'),\n",
       " ('distracted', 'from', 'this'),\n",
       " ('from', 'this', 'movie'),\n",
       " ('this', 'movie', '.'),\n",
       " ('movie', '.', 'Yet'),\n",
       " ('.', 'Yet', ','),\n",
       " ('Yet', ',', 'i'),\n",
       " (',', 'i', 'need'),\n",
       " ('i', 'need', 'to'),\n",
       " ('need', 'to', 'get'),\n",
       " ('to', 'get', 'work'),\n",
       " ('get', 'work', 'done'),\n",
       " ('work', 'done', '.'),\n",
       " ('done', '.', 'I'),\n",
       " ('.', 'I', 'wonder'),\n",
       " ('I', 'wonder', 'why'),\n",
       " ('wonder', 'why', 'one'),\n",
       " ('why', 'one', 'can'),\n",
       " ('one', 'can', 'not'),\n",
       " ('can', 'not', 'give')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(nltk.trigrams(new_text_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3570771a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', \"'m\", 'distracted', 'from', 'this'),\n",
       " (\"'m\", 'distracted', 'from', 'this', 'movie'),\n",
       " ('distracted', 'from', 'this', 'movie', '.'),\n",
       " ('from', 'this', 'movie', '.', 'Yet'),\n",
       " ('this', 'movie', '.', 'Yet', ','),\n",
       " ('movie', '.', 'Yet', ',', 'i'),\n",
       " ('.', 'Yet', ',', 'i', 'need'),\n",
       " ('Yet', ',', 'i', 'need', 'to'),\n",
       " (',', 'i', 'need', 'to', 'get'),\n",
       " ('i', 'need', 'to', 'get', 'work'),\n",
       " ('need', 'to', 'get', 'work', 'done'),\n",
       " ('to', 'get', 'work', 'done', '.'),\n",
       " ('get', 'work', 'done', '.', 'I'),\n",
       " ('work', 'done', '.', 'I', 'wonder'),\n",
       " ('done', '.', 'I', 'wonder', 'why'),\n",
       " ('.', 'I', 'wonder', 'why', 'one'),\n",
       " ('I', 'wonder', 'why', 'one', 'can'),\n",
       " ('wonder', 'why', 'one', 'can', 'not'),\n",
       " ('why', 'one', 'can', 'not', 'give')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(nltk.ngrams(new_text_tokens, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a0528505",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming \n",
    "from nltk.stem import PorterStemmer\n",
    "pst = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b406a954",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('win', 'studi')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pst.stem('winning'), pst.stem('studying')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "af222625",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\kawba\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\kawba\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "#lemmatization\n",
    "from nltk.stem import wordnet\n",
    "nltk.download('wordnet')\n",
    "#from nltk.stem import wordNetLemmatizer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3fca0647",
   "metadata": {},
   "outputs": [],
   "source": [
    "stem_this = ['cats', 'catchup', 'geese', 'cacti', 'studying']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "03995ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cats:cat\n",
      "catchup:catchup\n",
      "geese:goose\n",
      "cacti:cactus\n",
      "studying:studying\n"
     ]
    }
   ],
   "source": [
    "for i in stem_this:\n",
    "    print(i + ':' + lemmatizer.lemmatize(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8cbf3591",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parts of speech tagging (POS)\n",
    "\n",
    "korra = 'A stupid spoilt brat unworthy or Aang legacy. She lacks discernment and is impatient. Brash and incapable of wise judgement'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cde74d2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A',\n",
       " 'stupid',\n",
       " 'spoilt',\n",
       " 'brat',\n",
       " 'unworthy',\n",
       " 'or',\n",
       " 'Aang',\n",
       " 'legacy',\n",
       " '.',\n",
       " 'She',\n",
       " 'lacks',\n",
       " 'discernment',\n",
       " 'and',\n",
       " 'is',\n",
       " 'impatient',\n",
       " '.',\n",
       " 'Brash',\n",
       " 'and',\n",
       " 'incapable',\n",
       " 'of',\n",
       " 'wise',\n",
       " 'judgement']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "korra_tokens = word_tokenize(korra)\n",
    "korra_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dd131444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('A', 'DT')]\n",
      "[('stupid', 'JJ')]\n",
      "[('spoilt', 'NN')]\n",
      "[('brat', 'NN')]\n",
      "[('unworthy', 'JJ')]\n",
      "[('or', 'CC')]\n",
      "[('Aang', 'NN')]\n",
      "[('legacy', 'NN')]\n",
      "[('.', '.')]\n",
      "[('She', 'PRP')]\n",
      "[('lacks', 'NNS')]\n",
      "[('discernment', 'NN')]\n",
      "[('and', 'CC')]\n",
      "[('is', 'VBZ')]\n",
      "[('impatient', 'NN')]\n",
      "[('.', '.')]\n",
      "[('Brash', 'NN')]\n",
      "[('and', 'CC')]\n",
      "[('incapable', 'JJ')]\n",
      "[('of', 'IN')]\n",
      "[('wise', 'NN')]\n",
      "[('judgement', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "for i in korra_tokens:\n",
    "    print(nltk.pos_tag([i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e3b93e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\kawba\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\kawba\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#named entity recognition\n",
    "from nltk import ne_chunk\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "36e41a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "kawbabs = 'A sweet girl who sometimes works hard'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4edef077",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A', 'sweet', 'girl', 'who', 'sometimes', 'works', 'hard']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_tokens = word_tokenize(kawbabs)\n",
    "k_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "58f1b3fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', 'DT'),\n",
       " ('sweet', 'JJ'),\n",
       " ('girl', 'NN'),\n",
       " ('who', 'WP'),\n",
       " ('sometimes', 'RB'),\n",
       " ('works', 'VBZ'),\n",
       " ('hard', 'JJ')]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_tag = nltk.pos_tag(k_tokens)\n",
    "k_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d4e78b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
